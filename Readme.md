# ğŸ—‚ï¸ Project Folder Structure â€” Deep Learning Training Pipeline

This document provides an overview of the folder hierarchy, describing the purpose of each file and directory in the training pipeline.

---

## ğŸ“ Folder Layout

```
training_pipeline/
â”‚
â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€ Loads training & validation datasets using torchvision
â”‚   â”œâ”€ Applies GPU-accelerated transforms (if available)
â”‚   â””â”€ Returns DataLoaders + number of classes
â”‚
â”œâ”€â”€ model.py
â”‚   â”œâ”€ Builds the model architecture (ResNet-18)
â”‚   â””â”€ Replaces the final fully connected layer with custom num_classes
â”‚
â”œâ”€â”€ cyclic_scheduler.py
â”‚   â”œâ”€ Implements OneCycleLR scheduler
â”‚   â””â”€ Automatically sets LR schedule for each training step
â”‚
â”œâ”€â”€ lr_finder.py
â”‚   â”œâ”€ Implements a modern Learning Rate Finder (Smith 2017)
â”‚   â”œâ”€ Uses AMP + GradScaler for stability and speed
â”‚   â”œâ”€ Detects optimal LR and saves loss vs LR plots
â”‚   â””â”€ Restores model/optimizer automatically
â”‚
â”œâ”€â”€ train.py
â”‚   â”œâ”€ Main training pipeline
â”‚   â”œâ”€ Integrates LR Finder + OneCycleLR
â”‚   â”œâ”€ Tracks accuracy, loss, LR, and momentum
â”‚   â”œâ”€ Supports AMP, torch.compile(), and TF32 optimization
â”‚   â”œâ”€ Saves best and last model checkpoints
â”‚   â””â”€ Generates live plots for training progress
â”‚
â”œâ”€â”€ dataloader_optimization_readme.md
â”‚   â”œâ”€ Describes DataLoader performance tuning
â”‚   â””â”€ Includes benchmark results and parameter explanations
â”‚
â”œâ”€â”€ README_LR_FINDER.md
â”‚   â”œâ”€ Full documentation for the Learning Rate Finder module
â”‚   â””â”€ Explains usage, parameters, and interpretation
â”‚
â”œâ”€â”€ plots/
â”‚   â”œâ”€ lr_finder_plot.png
â”‚   â”œâ”€ accuracy_live.png
â”‚   â”œâ”€ loss_live.png
â”‚   â”œâ”€ lr_live.png
â”‚   â””â”€ momentum_live.png
â”‚
â”œâ”€â”€ training_log.txt
â”‚   â”œâ”€ CSV-formatted training history (Loss, Acc, LR, etc.)
â”‚
â”œâ”€â”€ best_weights.pth
â”‚   â””â”€ Best model checkpoint (highest validation accuracy)
â”‚
â””â”€â”€ last_weights.pth
    â””â”€ Last saved model after final epoch
```

---

## ğŸš€ Entry Point

| File | Purpose |
|------|----------|
| **`train.py`** | The **main entry point** for the entire pipeline. It orchestrates data loading, model creation, learning rate finding, scheduling, training, validation, and plotting. |

### â–¶ï¸ How to Run

You can launch the complete training workflow from the terminal:

```bash

# Run the main training script
python train.py
```

### Optional Arguments (to modify in script)

| Variable | Location | Description |
|-----------|-----------|-------------|
| `DATA_DIR` | inside `train.py` | Path to dataset root (`train/`, `val/` subfolders required) |
| `BATCH_SIZE` | inside `train.py` | Batch size for training and validation |
| `NUM_EPOCHS` | inside `train.py` | Total number of epochs |
| `DEVICE` | inside `train.py` | `"cuda"` or `"cpu"` |
| `PLOTS_DIR` | inside `train.py` | Directory to save plots (default: `"plots/"`) |

### Example Output

Running `python train.py` will:
- âœ… Run **Learning Rate Finder**  
- âš™ï¸ Configure **OneCycleLR scheduler**  
- ğŸ§  Train the **ResNet model**  
- ğŸ–¼ï¸ Generate **live accuracy/loss/LR plots**  
- ğŸ’¾ Save `best_weights.pth`, `last_weights.pth`, and `training_log.txt`

---

## ğŸ“– Overview

| Component | Description |
|------------|--------------|
| **Data Loader** | Efficient dataset loader with pinned memory, prefetching, and worker optimization. |
| **Model** | ResNet-based model with a replaceable classifier head. |
| **LR Finder** | Finds optimal learning rate using controlled exponential LR sweeps. |
| **Scheduler** | OneCycleLR policy for smooth convergence. |
| **Training** | Full AMP-optimized pipeline with progress tracking, checkpoints, and visualization. |
| **Documentation** | Markdown files describing optimization principles and LR Finder internals. |
| **Outputs** | Logs, plots, and weight checkpoints for reproducible experiments. |

---
