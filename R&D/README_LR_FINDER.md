# ğŸ“˜ Optimized Learning Rate Finder (`lr_finder.py`)

## ğŸ” Overview

The **Learning Rate Finder (LRFinder)** is a specialized diagnostic tool designed to automatically determine the most effective learning rate (LR) for training deep learning models.  
Instead of guessing, the LR Finder runs a brief training session where the learning rate increases gradually from a very low value to a high one, observing how the loss changes at each step.

The resulting **Learning Rate vs. Loss curve** helps pinpoint the optimal LR range â€” the sweet spot where learning is fast, stable, and efficient.

---

## ğŸ§© Approach and Workflow

The core idea is based on the **LR Range Test** (Smith, 2017).  
It performs a short run with exponentially or linearly increasing learning rates, and records how loss evolves.

| Step | Stage | Description |
|------|--------|-------------|
| 1ï¸âƒ£ | Initialize | Saves initial model & optimizer states |
| 2ï¸âƒ£ | Range Test | Gradually increases LR for `num_iter` iterations |
| 3ï¸âƒ£ | Loss Smoothing | Applies Savitzkyâ€“Golay or EMA to reduce noise |
| 4ï¸âƒ£ | Early Stop | Automatically halts if loss diverges too fast |
| 5ï¸âƒ£ | Analyze + Plot | Finds the steepest loss slope (best LR) |
| 6ï¸âƒ£ | Reset | Restores original model/optimizer weights |

---

## âš™ï¸ Key Features and Enhancements

| Feature | Description | Benefit |
|----------|--------------|----------|
| ğŸ§® **Mixed Precision** | Uses `torch.amp.autocast` & `GradScaler` | 2Ã— faster LR tests on GPU, lower VRAM usage |
| ğŸ§  **Savitzkyâ€“Golay Smoothing** | Uses polynomial filtering (if SciPy available) | Smooths loss curve, avoids spurious LR jumps |
| ğŸ’¾ **CSV Logging** | Saves LR/Loss data to `lr_finder_log.csv` | Enables reproducibility & analysis |
| ğŸ§¯ **Adaptive Early Stopping** | Detects explosive loss and halts automatically | Saves time and GPU cycles |
| ğŸ“ˆ **Gradient of log(Loss)** | Computes gradient on log-smoothed loss | Finds true steepest descent point |
| ğŸ¨ **Color Coded Plot Zones** | Visual zones: Yellow=Low, Green=Optimal, Red=High | Easier interpretation |
| â™»ï¸ **Auto Reset** | Restores model & optimizer after test | Prevents corrupting main training |
| ğŸ§© **Optional Dependencies** | Gracefully falls back if `pandas` or `scipy` missing | Portable to minimal environments |

---

## ğŸ§® Constructor and Parameters

```python
class LRFinder:
    def __init__(
        self,
        model,
        optimizer,
        criterion,
        device=None,
        memory_cache=True,
        cache_dir=None,
        scaler=None
    )
```

| Parameter | Description | Default |
|------------|--------------|----------|
| `model` | Model to test | Required |
| `optimizer` | Optimizer instance (SGD/Adam etc.) | Required |
| `criterion` | Loss function | Required |
| `device` | `"cuda"` or `"cpu"` | Inferred from model |
| `memory_cache` | Cache model & optimizer for reset | True |
| `cache_dir` | Optional path to store logs | None |
| `scaler` | Custom GradScaler for AMP | Auto-created |

---

## âš¡ `range_test()` â€” Core LR Sweep

```python
lr_finder.range_test(
    train_loader,
    start_lr=1e-7,
    end_lr=1,
    num_iter=100,
    step_mode="exp",
    smooth_f=0.05,
    diverge_th=5.0,
    use_amp=True,
    adaptive_stop=True
)
```

| Parameter | Purpose | Example | Notes |
|------------|----------|----------|-------|
| `train_loader` | DataLoader providing mini-batches | â€” | Required |
| `start_lr` | Starting LR | 1e-7 | Should be stable |
| `end_lr` | Final LR | 1 | Should cause divergence |
| `num_iter` | Iterations for LR test | 100 | Higher â†’ smoother |
| `step_mode` | LR increase pattern (`exp` / `linear`) | `"exp"` | `"exp"` is preferred |
| `smooth_f` | EMA smoothing factor | 0.05 | Reduces noise |
| `diverge_th` | Divergence threshold | 5.0 | Stops when unstable |
| `adaptive_stop` | Detects sudden spikes | True | Saves time |

---

## ğŸ“Š `plot()` â€” Visualization and Suggested LR

```python
best_lr = lr_finder.plot(
    save_path="plots/lr_finder_plot.png",
    suggest=True,
    save_csv=True,
    annotate=True,
    auto_reset=True
)
```

### Plot Details:
- **X-axis**: Learning Rate (log scale)
- **Y-axis**: Smoothed Loss
- **Red Dashed Line**: Suggested LR (steepest slope)
- **Color Zones**:
  - ğŸŸ¡ **Too Low LR** â†’ model learns too slowly
  - ğŸŸ¢ **Optimal Zone** â†’ loss drops rapidly & stable
  - ğŸ”´ **Too High LR** â†’ loss diverges / unstable

---

## ğŸ§  `get_suggested_lr()` â€” Direct LR Retrieval

If you want to skip plotting and just get the suggested LR:

```python
best_lr = lr_finder.get_suggested_lr()
```

Returns: Best LR found based on the minimum gradient of log-smoothed loss.

---

## ğŸ“ˆ Example Workflow

```python
from lr_finder import LRFinder

# 1. Initialize
lr_finder = LRFinder(model, optimizer, criterion, device="cuda")

# 2. Run range test
lr_finder.range_test(train_loader, start_lr=1e-7, end_lr=1, num_iter=100)

# 3. Plot & Get best LR
best_lr = lr_finder.plot(save_path="plots/lr_finder_plot.png")

# 4. Reset before training
lr_finder.reset()

print(f"Suggested LR: {best_lr:.6f}")
```

---

## ğŸ§© Outputs

| File | Description |
|------|--------------|
| `lr_finder_plot.png` | Learning Rate vs. Smoothed Loss curve |
| `lr_finder_log.csv` | Saved LRâ€“Loss pairs |
| `best_lr.txt` | Stores the best LR for later reuse |
| (Optional) Model restored to initial weights | Prevents corruption for next training |

---

## ğŸ” Example Terminal Log

```
Finding LR: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11<00:00, 9.18it/s]
âœ… LR range test complete! Use `.plot()` to visualize.
âœ… Suggested Learning Rate: 7.74E-03
ğŸ“ˆ LR Finder plot saved to plots/lr_finder_plot.png
ğŸ§¾ Saved lr history to lr_finder_log.csv
â™»ï¸ Model and optimizer state restored after LR test.
```

---

## ğŸ§­ Interpreting the Curve

| Region | Description | Interpretation |
|--------|--------------|----------------|
| ğŸŸ¡ **Too Low** | Loss almost flat | Model learns too slowly |
| ğŸŸ¢ **Optimal** | Loss drops steeply | Best LR range |
| ğŸ”´ **Too High** | Loss rises sharply | LR too large â€” unstable gradients |

The **optimal LR** is typically **just before** the loss starts to rise again.

---

## ğŸš€ Impact on Training

| Aspect | Before LR Finder | After LR Finder |
|---------|------------------|-----------------|
| LR Choice | Manual guess | Data-driven optimal |
| Convergence | Slow or oscillating | 2â€“3Ã— faster |
| Stability | Risk of divergence | Controlled and stable |
| Accuracy | Inconsistent | Early, smooth improvement |

---

## âš ï¸ Common Issues & Fixes

| Issue | Cause | Solution |
|--------|--------|----------|
| Loss diverges immediately | End LR too high | Lower `end_lr` (e.g., 1 â†’ 0.1) |
| Flat loss curve | Dataset too small | Increase `num_iter` |
| No suggested LR | Too few samples | Increase training batches |
| CUDA OOM | Batch size too large | Reduce `batch_size` temporarily |
| No Savitzky filter | SciPy missing | Auto-falls back to moving average |

---

## ğŸ’¡ Best Practices

âœ… Run LR Finder **before** every major training change (e.g., new model or dataset).  
âœ… Use **smaller batch sizes** for finer LR resolution.  
âœ… Apply the **suggested LR as `max_lr`** in `OneCycleLR`.  
âœ… Store the plot for **future reference and comparison**.

---

## ğŸ“œ References

- Leslie N. Smith, *"Cyclical Learning Rates for Training Neural Networks"*, 2017.  
- FastAIâ€™s `lr_find()` implementation (inspiration).  
- PyTorch LR Finder concept.

---

## ğŸ¯ Summary

The **Optimized LR Finder** automates learning rate selection using numerical stability, gradient-based detection, and loss smoothing.  
Itâ€™s **fast**, **AMP-optimized**, and **reproducible**, making it ideal for modern training pipelines.

âœ… Result: **Faster convergence, fewer experiments, and better models.**

---
