Excellent â€” thatâ€™s a key choice.
For **training ResNet-50 from scratch on ImageNet-1K**, the **learning-rate scheduler** you pick will strongly affect convergence speed and final accuracy.

Letâ€™s go through what works best **for modern setups (2020â€“2025)** and why ğŸ‘‡

---

## ğŸ” **Best Overall: Cosine Annealing with Warmup**

### âœ… **Recommended**

```python
import torch

warmup_epochs = 5
def cosine_lr_scheduler(optimizer, num_epochs, warmup_epochs=5):
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        return 0.5 * (1 + torch.cos(torch.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

### ğŸ’¡ Why itâ€™s ideal

* **Smooth decay**: avoids abrupt drops like StepLR.
* **Strong early learning** but **fine-tuning at end**, improving final accuracy.
* **Warmup** stabilizes gradients early when BatchNorm stats are noisy.
* Used by: **timm**, **DeiT**, **ConvNeXt**, **EfficientNet**, etc.
* Works beautifully with **SGD**, **AdamW**, or **Lion** optimizers.
* Boosts Top-1 by ~+0.3â€“0.5% over step decay.

---

## âš™ï¸ Typical hyperparameters

| Parameter     | Typical value           | Notes                 |
| ------------- | ----------------------- | --------------------- |
| Initial LR    | 0.1 for SGD (batch 256) | scale âˆ batch size    |
| Warmup epochs | 5â€“10                    | Linear increase       |
| Max epochs    | 100â€“300                 | Depends on compute    |
| Min LR        | 1e-6                    | Donâ€™t let it hit zero |

---

## âš–ï¸ **Alternatives (if you want variations)**

### 1ï¸âƒ£ **One-Cycle LR** (`torch.optim.lr_scheduler.OneCycleLR`)

* Great if you use **AdamW**.
* Rapid rise â†’ slow decay schedule.
* Works well for shorter runs (â‰¤100 epochs).
* May need fine-tuned max_lr to avoid instability.

### 2ï¸âƒ£ **StepLR (Legacy)**

* Used in original ResNet papers.
* Decays LR by 0.1 every 30 epochs.
* **Outdated** now â€” abrupt jumps can hurt convergence stability.

### 3ï¸âƒ£ **Cosine Annealing with Restarts (SGDR)**

* Periodically resets LR to high value (Tâ‚€, T_mult).
* Can slightly improve generalization but adds complexity.
* Rarely used for full ImageNet runs now.

---

## ğŸš€ **Summary Recommendation**

| Use case                      | Scheduler                      | Why                                        |
| ----------------------------- | ------------------------------ | ------------------------------------------ |
| Standard ImageNet-1K training | **CosineAnnealingLR + Warmup** | Best balance of speed, stability, accuracy |
| Short or fine-tuning runs     | **OneCycleLR**                 | Faster convergence                         |
| Legacy baseline reproduction  | **StepLR (30, 0.1)**           | Matches old ResNet papers                  |

---

âœ… **Verdict:**

> For your **ResNet-50 ImageNet-1K training from scratch**, use
> **Cosine Annealing with 5â€“10 epoch warmup** and **final LR floor â‰ˆ 1e-6**.

